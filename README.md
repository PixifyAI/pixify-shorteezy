# Shorteezy

![Shorteezy](shorteezy.png)

# AI-Powered YouTube/TikTok Short Generator

## Table of Contents
1. [Project Overview](#project-overview)
2. [Features](#features)
3. [Prerequisites](#prerequisites)
4. [Installation](#installation)
5. [Usage](#usage)
6. [Project Structure](#project-structure)
7. [Configuration](#configuration)
8. [Troubleshooting](#troubleshooting)
9. [Contributing](#contributing)
10. [License](#license)

## Project Overview

This project is an AI-powered YouTube Short/TikTok generator that takes source material or url as input and creates engaging short-form video content. It utilizes various AI technologies for local llm script generation, local TTS text-to-speech conversion, FastFlux api image generation, and local cv2 video composition. CLI works but webui is still in progress.

## Features

- AI-generated script based on provided source material or url useing LM Studio server for local free llm
- Text-to-speech narration useing basic TTS , will update to better voice soon
- AI-generated images for visual content useing runware.ai with less than 3 second flux.dev generation
- Automatic video composition with image transitions useing OpenCV cv2
- Text overlay on video for captions with cv2

## Prerequisites

- Python 3.7+
- FFmpeg (for video processing)
- LMStudio (for local AI model)
- Runware.ai account setup for api (for fast flux image generation, free $15 to start and will make 1000 512x512 images for about $1)

## Installation

1. Clone the repository:
   ```
   git clone https://github.com/PixifyAI/shorteezy-webui.git
   cd shorteezy-webui
   ```

2. Set up a virtual environment (optional but recommended):
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows use, venv\Scripts\activate
   ```

3. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

4. Set up environment variables:
   Create a `.env` file in the project root and add the following:
   ```
   RUNWARE_API_KEY=your_runware_api_key_here
   ```

## Usage

1. Prepare your source material in input.txt text file or use url.

2. Make sure your LM Studio server is running and add your model name on line 71 in main.py

3. Setup user on https://runware.ai/ and make an API key, add your api key to .env file. ($15 free credit and $0.0077 per image)

4. Run the main script:

   python main.py input.txt [settings_file.json]

   python main.py url [settings_file.json]  (with full http prefix) 

5. It will generate a response.txt in shorts\1725331787(random # gen) and will pause with

   Press Enter to continue when you're done editing...

   edit the txt if not good info from llm, MUST be in the below format and some open llm models dont do a great job

###

[image gen info description]

Narrator: "Text generated by llm to be read."

[image gen info description]

Narrator: "Text generated by llm to be read."

[image gen info description]

Narrator: "Text generated by llm to be read."

###

6. Once done and happy with response.txt press Enter to continue

7. The generated video will be saved in the `shorts/{timestamp}` directory.

## Project Structure

- `main.py`: Main script that orchestrates the entire process
- `images.py`: Handles AI image generation
- `narration.py`: Manages text-to-speech conversion
- `text.py`: Adds text overlays to video
- `video.py`: Creates the final video with transitions
- `requirements.txt`: List of Python dependencies
- `input.txt`: content you want the llm to summerize
- `.env`: runware api key
- `README.md`: install and setup info
- `webui.py`: Gradio webui for UI
## Configuration

You can customize the video generation process by creating a `settings.json` file. Example structure:

```json
{
  "caption_settings": {
    "font": "Arial",
    "font_size": 24,
    "color": "white"
  }
}
```

Pass this file as the second argument when running `main.py`.


## WebUI

1. Launch the webui:
   ```
   python webui.py
   ```
   you will get a link in the prompt http://127.0.0.1:7860/

## Troubleshooting
- venv is a virtual enviroment so you dont have conflicting dependancies, should use for easy install.
- If you encounter issues with image generation, ensure your Runware API key is correctly set in the `.env` file.
- For text-to-speech problems, check that the TTS model is correctly installed and accessible.
- If video composition fails, verify that FFmpeg is correctly installed and accessible from the command line.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

For any questions or support, please open an issue on the GitHub repository.
